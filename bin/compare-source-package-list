#!/usr/bin/python

# Copyright 2011 Paul Wise
# Released under the MIT/Expat license, see doc/COPYING

# Uses the snapshot.debian.org metadata database and SHA-1 based filesystem to
# compute debdiffs between Debian and individual derivatives. The metadata
# allows knowing if a particular file was ever in Debian and the filesystem
# allows the creation of debdiffs.
#
# The script works approximately like this:
#
# Load the Sources files previously downloaded by get-package-lists as indicated
# by the sources.list of the derivative.
#
# For each source package in the derivative:
#
# Check if the dsc has ever been in Debian, if not, check if the other
# parts have and therefore decide if the package is unmodified or not.
# Unmodified source packages are skipped and include those with the exact
# same dsc file or those where all the non-dsc parts are identical.
#
# Try some heuristics (name, version, changelog entries) to find out if
# the package could be based on some package that is or was in Debian.
#
# If it was not then skip to the next one and make a note, since Debian
# might want to know about source packages that are missing from Debian.
#
# If it was then use debdiff to create a diff and filterdiff to create a
# diff of the debian/ dir.
#
# Usage:
# compare-source-package-list <sources.list> <apt dir> <patches list> <links list> <new package list> <log file>

# FIXME: write out some statistics and rrdtool graphs
#		source package types per derivative
#		number of source packages
#		cache misses: md5, sha256, sha1, patch, changelog
# FIXME: comment the code to list assumptions and function purpose
# FIXME: add options to allow re-processing only specific packages
# FIXME: write something to clean up old files and patches
# FIXME: don't unpack or make a patch when we don't have all the parts
# FIXME: don't make a patch when we were not able to unpack the source package
# FIXME: deal with permissions errors on unpack??
# FIXME: skip lzip encoded files??
# FIXME: cleanup files at start of run
# FIXME: extract new debian/patches/ patches
# FIXME: print out packages that are no longer in Debian
# FIXME: deal with really large patches:
# FIXME:   kde-l10n-*: too few parts to be useful
# FIXME:   divergence: too many changelog entries between versions to be useful
# FIXME:   derivative is older than Debian
# FIXME:   derivative renamed the source package
# FIXME:   just a really big diff
# FIXME: when there are multiple dsc files in snapshots, prefer the debian/debian-archive one
# FIXME: when the source package is ancient and the dsc is missing, make a fake one to use
# FIXME: add an in-memory cache of hashes so that hashes in multiple releases hit the disk once

import os
import sys
import httplib
import urllib2
import hashlib
import shutil
import logging
import tempfile
import string
import socket
import signal
import subprocess
import yaml
from debian import deb822, changelog
import apt_pkg
import psycopg2
try: import cjson as json
except ImportError: import json

# Helper functions for python stuff with annoying error handling

def makedirs(dirs):
	try: os.makedirs(dirs)
	except OSError: pass

def rmtree(dir):
	try: shutil.rmtree(dir)
	except OSError: pass

def remove(file):
	try: os.remove(file)
	except OSError: pass

def symlink(source, link):
	try: os.symlink(source, link)
	except OSError: pass

# http://www.chiark.greenend.org.uk/ucgi/~cjwatson/blosxom/2009-07-02-python-sigpipe.html
def subprocess_setup():
    # Python installs a SIGPIPE handler by default. This is usually not what
    # non-Python subprocesses expect.
    signal.signal(signal.SIGPIPE, signal.SIG_DFL)

# Config
md5_cache_dir = os.path.abspath('../md5-farm')
sha1_cache_dir = os.path.abspath('../sha1-farm')
sha256_cache_dir = os.path.abspath('../sha256-farm')
sha1_patch_dir = os.path.abspath('../sha1-patches')
sha1_lsdiff_dir = os.path.abspath('../sha1-lsdiff')
sha1_changelog_dir = os.path.abspath('../sha1-changelog')
deriv_patch_dir = os.path.abspath('patches')
global_patch_dir = os.path.abspath('../patches')
snapshot_cache_dir = '/srv/snapshot.debian.org/farm'
patch_too_large = os.path.abspath('../../doc/patch-too-large.txt')
checksum_headers = ('Checksums-Sha1', 'Checksums-Sha256', 'Files')
user_agent = 'Debian Derivatives Census QA bot'
timeout = 60
ishex = lambda s: not(set(s)-set(string.hexdigits))

# Init
apt_pkg.init()

# Setup configuration
apt_pkg.config.set('Dir', os.path.abspath(sys.argv[2]))
apt_pkg.config.set('Dir::Etc', os.path.abspath(sys.argv[2]))
apt_pkg.config.set('Dir::State', os.path.abspath(sys.argv[2]))
apt_pkg.config.set('Dir::Cache', os.path.abspath(sys.argv[2]))
apt_pkg.config.set('Dir::State::status', os.path.abspath(os.path.join(sys.argv[2],'status')))
apt_pkg.config.set('Dir::Etc::sourcelist', os.path.abspath(sys.argv[1]))
apt_pkg.config.set('Dir::Bin::gpg', 'fakegpgv')

# Preparation
sources_list = apt_pkg.SourceList()
sources_list.read_main_list()
conn = psycopg2.connect("service=snapshot-guest")
cur = conn.cursor()
remove(sys.argv[7])
logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG, filename=sys.argv[7])

# Voodoo
lists_dir = apt_pkg.config.find_dir('Dir::State::lists')
source_entries = [[i for i in x.index_files if i.label=='Debian Source Index'] for x in sources_list.list]
derivative_short_name = os.path.basename(os.getcwd())
modifies_dsc_files = 0
repackaged_but_identical = 0

# Helper functions for generating path names

def hash_path_parent(dir, hash):
	return os.path.join(dir, hash[0:2], hash[2:4])

def hash_path(dir, hash):
	return os.path.join(dir, hash[0:2], hash[2:4], hash)

def hash_path_exists(dir, hash):
	return os.path.exists(os.path.join(dir, hash[0:2], hash[2:4], hash))

def sha1_patch_path(debian_dsc_sha1, dsc_sha1, type=None):
	path = os.path.join(hash_path(sha1_patch_dir, debian_dsc_sha1), hash_path('', dsc_sha1))
	if type: path += '.%s' % type
	path += '.patch'
	return os.path.abspath(path)

def sha1_lsdiff_path(debian_dsc_sha1, dsc_sha1, type=None):
	path = os.path.join(hash_path(sha1_lsdiff_dir, debian_dsc_sha1), hash_path('', dsc_sha1))
	if type: path += '.%s' % type
	path += '.lsdiff'
	return os.path.abspath(path)

def shortslug(name):
	return name[:4] if name.startswith('lib') else name[0]

def deriv_patch_path(name, version, debian_name, debian_version, type=None):
	path = os.path.join(deriv_patch_dir, shortslug(debian_name), debian_name, '')
	path += '_'.join((debian_name, debian_version, name, version))
	if type: path += '.%s' % type
	path += '.patch'
	return os.path.abspath(path)

def global_patch_path(name, version, debian_name, debian_version, type=None):
	path = os.path.join(global_patch_dir, shortslug(debian_name), debian_name, '')
	path += '_'.join(('Debian', debian_name, debian_version, derivative_short_name, name, version))
	if type: path += '.%s' % type
	path += '.patch'
	return os.path.abspath(path)

# Functions for downloading files and storing them in the hash caches

def download_and_check_hash(url, dir, hash, hash_type):
	try:
		parent = hash_path_parent(dir,hash)
		path = hash_path(dir,hash)
		logging.debug('downloading %s', url)
		makedirs(parent)
		headers = { 'User-Agent' : user_agent }
		req = urllib2.Request(url, None, headers)
		u = urllib2.urlopen(req, None, timeout)
		data = u.read()
		if hash_type == 'sha256':
			data_hash = hashlib.sha256(data).hexdigest()
		elif hash_type == 'md5sum':
			data_hash = hashlib.md5(data).hexdigest()
		else:
			logging.warning('unknown hash type detected: %s %s %s', hash_type, hash, url)
			return ('unknown', None)
		if data_hash != hash:
			logging.warning('incorrect hash for downloaded file, ignoring: %s %s != %s %s', hash_type, hash, data_hash, url)
			return ('unknown', None)
		sha1 = hashlib.sha1(data).hexdigest()
		sha1_path = hash_path(sha1_cache_dir, sha1)
		sha1_parent = hash_path_parent(sha1_cache_dir, sha1)
		makedirs(sha1_parent)
		if hash_path_exists(snapshot_cache_dir, sha1):
			snapshot_path = hash_path(snapshot_cache_dir, sha1)
			symlink(snapshot_path, path)
			logging.debug('exists in snapshot sha1 cache: %s %s %s %s', hash_type, hash, sha1, url)
			return (True, sha1)
		else:
			if not os.path.exists(sha1_path):
				logging.debug('correct hash for downloaded file, saving: %s %s %s %s', hash_type, hash, sha1, url)
				f = open(sha1_path, 'w')
				f.write(data)
				f.close()
			else:
				logging.debug('correct hash for downloaded file, not saving: already in derivs cache: %s %s %s %s', hash_type, hash, sha1, url)
			symlink(os.path.relpath(sha1_path, os.path.dirname(path)), path)
			logging.debug('does not exist in snapshot sha1 cache: %s %s %s %s', hash_type, hash, sha1, url)
			return (False, sha1)
	except urllib2.URLError, e:
		if hasattr(e, 'reason'): reason = e.reason
		elif hasattr(e, 'code'): reason = e.code
		else: reason = e
		logging.warning('unable to download hash file, ignoring: %s %s', reason, url)
		return ('unknown', None)
	except httplib.HTTPException, e:
		logging.warning('unable to download hash file, ignoring: %s %s', repr(e), url)
		return ('unknown', None)
	except socket.error, e:
		logging.warning('unable to download hash file, ignoring: %s %s', e, url)
		return ('unknown', None)

def download_sha1(url, dir, sha1):
	try:
		parent = hash_path_parent(dir,sha1)
		path = hash_path(dir,sha1)
		logging.debug('downloading sha1: %s %s', sha1, url)
		makedirs(parent)
		headers = { 'User-Agent' : user_agent }
		req = urllib2.Request(url, None, headers)
		u = urllib2.urlopen(req, None, timeout)
		data = u.read()
		data_sha1 = hashlib.sha1(data).hexdigest()
		if data_sha1 == sha1:
			logging.debug('correct sha1 for downloaded file, saving: %s %s', sha1, url)
			if not os.path.exists(path):
				f = open(path, 'w')
				f.write(data)
				f.close()
			return (False, sha1)
		else:
			logging.warning('incorrect sha1 for downloaded file, ignoring: %s != %s %s', sha1, data_sha1, url)
			return ('unknown', None)
	except urllib2.URLError, e:
		if hasattr(e, 'reason'): reason = e.reason
		elif hasattr(e, 'code'): reason = e.code
		else: reason = e
		logging.warning('unable to download sha1 file, ignoring: %s %s', reason, url)
		return ('unknown', None)
	except httplib.HTTPException, e:
		logging.warning('unable to download hash file, ignoring: %s %s', repr(e), url)
		return ('unknown', None)
	except socket.error, e:
		logging.warning('unable to download hash file, ignoring: %s %s', e, url)
		return ('unknown', None)

# Functions for checking the hash caches

def check_hash_cache(dir, hash, hash_type, url):
	logging.debug('checking hash cache: %s %s', hash_type, hash)
	path = hash_path(dir, hash)
	try:
		result = os.readlink(path)
		path = os.path.join(os.path.dirname(path), result)
	except OSError:
		logging.debug('does not exist in hash cache: %s %s', hash_type, hash)
		return download_and_check_hash(url, dir, hash, hash_type)
	logging.debug('exists in hash cache: %s %s', hash_type, hash)
	sha1 = os.path.basename(path)
	if hash_path_exists(snapshot_cache_dir, sha1):
		logging.debug('exists in snapshot sha1 cache: %s', sha1)
		remove(hash_path(sha1_cache_dir,sha1))
		return (True, sha1)
	elif hash_path_exists(sha1_cache_dir, sha1):
		logging.debug('exists in derivatives sha1 cache: %s', sha1)
		return (False, sha1)

def check_sha1_cache(sha1, url):
	logging.debug('checking sha1 caches: %s', sha1)
	if hash_path_exists(snapshot_cache_dir, sha1):
		logging.debug('exists in snapshot sha1 cache: %s', sha1)
		remove(hash_path(sha1_cache_dir,sha1))
		return (True, sha1)
	elif hash_path_exists(sha1_cache_dir, sha1):
		logging.debug('exists in derivatives sha1 cache: %s', sha1)
		return (False, sha1)
	else:
		logging.debug('does not exist in any sha1 caches: %s', sha1)
		return download_sha1(url, sha1_cache_dir, sha1)

def status(type, hash, url):
	logging.debug('checking status of hash: %s %s %s', type, hash, url)
	if type == 'sha1':
		(ret, sha1) = check_sha1_cache(hash, url)
		if ret == True:
			return ('unmodified', sha1)
		elif ret == False:
			return ('modified', sha1)
		else:
			return (ret, sha1)
	elif type == 'sha256':
		(ret, sha1) = check_hash_cache(sha256_cache_dir, hash, type, url)
		if ret == True:
			return ('unmodified', sha1)
		elif ret == False:
			return ('modified', sha1)
		else:
			return (ret, sha1)
	elif type == 'md5sum':
		(ret, sha1) = check_hash_cache(md5_cache_dir, hash, type, url)
		if ret == True:
			return ('unmodified', sha1)
		elif ret == False:
			return ('modified', sha1)
		else:
			return (ret, sha1)
	else:
		logging.warning('unknown hash type detected: %s %s %s', type, hash, url)
		return ('unknown', None)

# Functions for getting information about source packages

def get_info(srcpkg):
	dsc = None
	for header in checksum_headers:
		if not dsc and header in srcpkg:
			dsc = [x for x in srcpkg[header] if x['name'].endswith('.dsc')]
	if not dsc:
		logging.warning('did not find any dsc files')
		return None
	if len(dsc) > 1:
		logging.warning('found multiple dsc files: %s' % ' '.join(['%s %s' % (dsc_name, dsc_sha1) for dsc_name, dsc_sha1 in dsc]))
		return None
	dsc = dsc[0]
	dsc_name = dsc['name']
	dsc_hash_type, dsc_hash =  [(k, v) for k, v in dsc.iteritems() if k not in ('name', 'size')][0]

	parts = []
	part_names = []
	for header in checksum_headers:
		if header in srcpkg:
			for part in srcpkg[header]:
				if 'name' in part and part['name'] not in part_names and not part['name'].endswith('.dsc'):
					parts.append(part)
					part_names.append(part['name'])

	return (dsc_hash_type, dsc_hash, dsc_name, parts)

def get_debian_info(files):
	dsc = [file for file in files if file[0].endswith('.dsc')]
	if not dsc:
		logging.warning('did not find any Debian dsc files: snapshots bug or ancient source package')
		return None
	if len(dsc) > 1:
		logging.warning('found multiple Debian dsc files, choosing first one: %s' % ' '.join(['%s %s' % (dsc_name, dsc_sha1) for dsc_name, dsc_sha1 in dsc]))

	dsc = dsc[0]
	dsc_name, dsc_sha1 = dsc

	parts = []
	part_names = []
	for file in files:
		part_name, part_sha1 = file
		if part_name not in part_names and not part_name.endswith('.dsc'):
			parts.append(file)
			part_names.append(part_name)

	return (dsc_sha1, dsc_name, parts)

# Functions for extracting information from the snapshots database

def database_error(e):
	reason = None
	code = None
	if hasattr(e, 'pgerror'): reason = e.pgerror
	if hasattr(e, 'pgcode'): code = e.pgcode
	logging.warning('unable to execute database query: %s %s', code, reason)
	conn.reset()

def srcpkg_was_in_debian(name, version=None):
	try:
		if version:
			cur.execute('SELECT version FROM srcpkg WHERE name=%s AND version=%s LIMIT 1;', (name, version))
			return not not cur.fetchone()
		else:
			cur.execute('SELECT version FROM srcpkg WHERE name=%s LIMIT 1;', (name,))
			return not not cur.fetchone()
	except psycopg2.Error, e:
		database_error(e)
		return None

def sha1_to_srcpkgs(sha1):
	try:
		cur.execute(
			'''SELECT name, version
			FROM srcpkg
			JOIN file_srcpkg_mapping ON file_srcpkg_mapping.srcpkg_id=srcpkg.srcpkg_id
			WHERE hash=%s;''', (sha1,))
		return cur.fetchall()
	except psycopg2.Error, e:
		database_error(e)
		return None

def srcpkg_to_sha1s(name, version):
	try:
		cur.execute(
			'''SELECT hash
			FROM file_srcpkg_mapping
			JOIN srcpkg ON srcpkg.srcpkg_id=file_srcpkg_mapping.srcpkg_id
			WHERE name=%s AND version=%s;''', (name, version))
		return cur.fetchall()
	except psycopg2.Error, e:
		database_error(e)
		return None

def srcpkg_to_srcpkgs(name):
	try:
		cur.execute(
			'''SELECT name, version
			FROM srcpkg
			WHERE name=%s ORDER BY version DESC;''', (name,))
		return cur.fetchall()
	except psycopg2.Error, e:
		database_error(e)
		return None

def sha1s_to_files(sha1):
	try:
		cur.execute('SELECT DISTINCT ON (name, hash) name, hash FROM file WHERE hash=%s;', hash)
		return cur.fetchall()
	except psycopg2.Error, e:
		database_error(e)
		return None

def srcpkg_to_files(name, version):
	try:
		cur.execute(
			'''SELECT DISTINCT ON (file.name, file.hash) file.name, file.hash
			FROM file_srcpkg_mapping
			JOIN srcpkg ON srcpkg.srcpkg_id=file_srcpkg_mapping.srcpkg_id
			JOIN file ON file_srcpkg_mapping.hash=file.hash
			WHERE srcpkg.name=%s AND srcpkg.version=%s;''', (name, version))
		return cur.fetchall()
	except psycopg2.Error, e:
		database_error(e)
		return None

def sha1_version_to_derived_from(sha1, version):
	try:
		cur.execute(
			'''SELECT name, version
			FROM srcpkg
			JOIN file_srcpkg_mapping ON file_srcpkg_mapping.srcpkg_id=srcpkg.srcpkg_id
			WHERE hash=%s and version<=%s
			ORDER BY name ASC, version DESC
			LIMIT 1;''', (sha1, version))
		res = cur.fetchall()
		if res: return res
		cur.execute(
			'''SELECT name, version
			FROM srcpkg
			JOIN file_srcpkg_mapping ON file_srcpkg_mapping.srcpkg_id=srcpkg.srcpkg_id
			WHERE hash=%s
			ORDER BY name ASC, version ASC
			LIMIT 1;''', (sha1, version))
		return cur.fetchall()
	except psycopg2.Error, e:
		database_error(e)
		return None

def srcpkg_to_derived_from(name, version):
	try:
		cur.execute(
			'''SELECT name, version
			FROM srcpkg
			WHERE name=%s and version<=%s
			ORDER BY version DESC
			LIMIT 1;''', (name, version))
		res = cur.fetchall()
		if res: return res
		cur.execute(
			'''SELECT name, version
			FROM srcpkg
			WHERE name=%s
			ORDER BY version ASC
			LIMIT 1;''', (name,))
		return cur.fetchall()
	except psycopg2.Error, e:
		database_error(e)
		return None

# Functions related to creating patches

# Add symlinks for all needed files
def prepare(dsc_name, dsc_sha1, parts):
	logging.debug('preparing deriv directory for %s', dsc_name)
	tmp_dir = tempfile.mkdtemp(prefix='derivs-cmp-srcpkg-%s-' % derivative_short_name)
	path = hash_path(snapshot_cache_dir, dsc_sha1)
	if not os.path.exists(path): path = hash_path(sha1_cache_dir, dsc_sha1)
	path = hash_path(sha1_cache_dir, dsc_sha1)
	dsc_path = os.path.join(tmp_dir, dsc_name)
	os.symlink(path, dsc_path)
	for part in parts:
		if 'sha1' in part:
			path = hash_path(snapshot_cache_dir, part['sha1'])
			if not os.path.exists(path): path = hash_path(sha1_cache_dir, part['sha1'])
		elif 'sha256' in part: path = hash_path(sha256_cache_dir, part['sha256'])
		elif 'md5sum' in part: path = hash_path(md5_cache_dir, part['md5sum'])
		part_path = os.path.join(tmp_dir, part['name'])
		os.symlink(path, part_path)
	return tmp_dir

def prepare_debian(dsc_name, dsc_sha1, files):
	logging.debug('preparing Debian directory for %s', dsc_name)
	debian_tmp_dir = tempfile.mkdtemp(prefix='derivs-cmp-srcpkg-Debian-')
	path = hash_path(snapshot_cache_dir, dsc_sha1)
	dsc_path = os.path.join(debian_tmp_dir, dsc_name)
	os.symlink(path, dsc_path)
	for file in files:
		part_name, part_sha1 = file
		path = hash_path(snapshot_cache_dir, part_sha1)
		part_path = os.path.join(debian_tmp_dir, part_name)
		os.symlink(path, part_path)
	return debian_tmp_dir

def get_changelog_entries(tmp_dir, dsc_name, dsc_sha1):
	logging.debug('getting changelog entries from %s', dsc_name)

	# Cache check
	changelog_path = hash_path(sha1_changelog_dir, dsc_sha1)
	if os.path.exists(changelog_path):
		logging.debug('changelog cache exists for %s %s', dsc_name, dsc_sha1)
		f = file(changelog_path)
		changelog_entries = json.load(f)
		changelog_entries = [tuple(entry) for entry in changelog_entries]
		f.close()
		return changelog_entries

	# Preparation
	extract_path = os.path.join(tmp_dir,'extracted')

	# Unpack the source tree
	logging.debug('unpacking source package %s', dsc_name)
	cmdline = ['dpkg-source', '-x', dsc_name, 'extracted']
	process = subprocess.Popen(cmdline, cwd=tmp_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, preexec_fn=subprocess_setup)
	output = process.communicate()[0]
	if process.returncode:
		logging.warning('dpkg-source reported failure to extract %s:', dsc_name)
		logging.warning(output)
		cmdline = ['ls', '-lR', '--time-style=+']
		process = subprocess.Popen(cmdline, cwd=tmp_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, preexec_fn=subprocess_setup)
		output = process.communicate()[0]
		logging.warning(output)
		rmtree(extract_path)
		return None

	# Sanitise the debian dir and changelog file in case it is a symlink to outside
	debian_dir = os.path.join(extract_path, 'debian')
	changelog_filename = os.path.join(debian_dir,'changelog')
	if os.path.islink(debian_dir) or os.path.islink(changelog_filename):
		logging.warning('debian dir or changelog is a symbolic link %s', dsc_name)
		rmtree(extract_path)
		return None

	# Check if the changelog exists
	if not os.path.exists(changelog_filename):
		logging.warning('could not find changelog in %s', dsc_name)
		rmtree(extract_path)
		return None

	# Find out which source package is the most likely derivative
	logging.debug('parsing changelog for %s', dsc_name)
	changelog_file = open(changelog_filename)
	changelog_obj = changelog.Changelog(changelog_file)
	try:
		changelog_entries = [(entry.package, str(entry._raw_version)) for entry in changelog_obj]
	except:
		logging.warning('could not read changelog from %s', dsc_name)
		rmtree(extract_path)
		return None
	del changelog_obj
	changelog_file.close()

	# Clean up again
	rmtree(extract_path)

	# Write the cache
	makedirs(hash_path_parent(sha1_changelog_dir, dsc_sha1))
	f = file(changelog_path, 'w')
	json.dump(changelog_entries, f)
	f.close()

	return changelog_entries

# Find the source package name and version this is probably derived from
def find_derived_from(tmp_dir, name, version, dsc_name, dsc_sha1, parts_unmodified):
	logging.debug('finding base source package of %s %s', name, version)

	# Get a list of changelog entries
	changelog_entries = get_changelog_entries(tmp_dir, dsc_name, dsc_sha1)
	if changelog_entries:
		logging.debug('changelog entries are: %s', ' '.join(['%s %s' % (entry_name, entry_version) for entry_name, entry_version in changelog_entries]))

	# Get a list of candidate versions from the database
	possibly_derived_from = []
	logging.debug('checking which parts were in Debian')
	for part_sha1, part_name in parts_unmodified:
		part_derived_from = sha1_to_srcpkgs(part_sha1)
		if part_derived_from:
			logging.debug('part %s %s available in %s', part_sha1, part_name, ' '.join(['%s %s' % (entry_name, entry_version) for entry_name, entry_version in part_derived_from]))
			possibly_derived_from.extend(part_derived_from)

	if not possibly_derived_from:
		logging.debug('no parts in common with Debian, obtaining old versions')
		old_packages = srcpkg_to_srcpkgs(name)
		if old_packages: possibly_derived_from = old_packages

	# Uniqify
	possibly_derived_from = list(set(possibly_derived_from))
	if possibly_derived_from:
		logging.debug('possibly derived from: %s', ' '.join(['%s %s' % (entry_name, entry_version) for entry_name, entry_version in possibly_derived_from]))
	else:
		logging.debug('nothing in possibly derived from list')

	# Match changelog versions against candidates
	if changelog_entries:
		logging.debug('matching changelog entries against versions possibly derived from')
		for entry in changelog_entries:
			entry_name, entry_version = entry
			if entry in possibly_derived_from:
				logging.debug('%s %s in possibly derived from', entry_name, entry_version)
				return entry
		logging.debug('checking if changelog entries were ever in Debian')
		for entry_name, entry_version in changelog_entries:
			if srcpkg_was_in_debian(entry_name, entry_version):
				logging.debug('%s %s was in Debian', entry_name, entry_version)
				return (entry_name, entry_version)
	if possibly_derived_from:
		logging.debug('finding closest entry in possibly derived from')
		possibly_derived_from.sort(cmp=lambda a,b: apt_pkg.version_compare(b[1],a[1]))
		for entry_name, entry_version in possibly_derived_from:
			if name == entry_name and apt_pkg.version_compare(version, entry_version) <= 0:
				logging.debug('%s %s is a lower version', entry_name, entry_version)
				return (entry_name, entry_version)
		entry = possibly_derived_from[-1]
		entry_name, entry_version = entry
		logging.debug('no lower version numbers, returning next highest version %s %s', entry_name, entry_version)
		return entry
	logging.debug('finding closest version number in Debian')
	for entry in srcpkg_to_derived_from(name, version):
		entry_name, entry_version = entry
		logging.debug('closest package was %s %s', entry_name, entry_version)
		return entry
	logging.debug('could not find Debian package %s %s is derived from', name, version)
	return None

# Generate a patch file
def create_patch(tmp_dir, dsc_name, dsc_sha1, debian_tmp_dir, debian_dsc_name, debian_dsc_sha1):
	global repackaged_but_identical

	dsc_path = os.path.join(tmp_dir, dsc_name)
	debian_dsc_path = os.path.join(debian_tmp_dir, debian_dsc_name)
	path_everything = sha1_patch_path(debian_dsc_sha1, dsc_sha1)
	path_debian = sha1_patch_path(debian_dsc_sha1, dsc_sha1, 'debian')
	path_filtered = sha1_patch_path(debian_dsc_sha1, dsc_sha1, 'filtered')
	path_debian_filtered = sha1_patch_path(debian_dsc_sha1, dsc_sha1, 'debian-filtered')

	# Generate the main patch
	if not os.path.exists(path_everything) and os.path.exists(debian_dsc_path) and os.path.exists(dsc_path):
		makedirs(os.path.dirname(path_everything))
		cmdline = ['debdiff', '--quiet', '--diffstat', debian_dsc_path, dsc_path]
		stdout = open(path_everything, 'w')
		process = subprocess.Popen(cmdline, stdout=stdout, stderr=subprocess.PIPE, preexec_fn=subprocess_setup)
		output = process.communicate()[1]
		stdout.close()
		if process.returncode == 255:
			logging.warning('debdiff reported failure %s %s:', debian_dsc_name, dsc_name)
			logging.warning(output)
			cmdline = ['ls', '-lR', '--time-style=+']
			for name, dir in (derivative_short_name, tmp_dir), ('Debian', debian_tmp_dir):
				logging.warning('dir listing for %s:', name)
				process = subprocess.Popen(cmdline, cwd=dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, preexec_fn=subprocess_setup)
				output = process.communicate()[0]
				logging.warning(output)
			return False
		elif process.returncode == 0:
			logging.info('derivative repackaged in an identical way %s %s', debian_dsc_name, dsc_name)
			repackaged_but_identical += 1
			return False
		elif process.returncode != 1:
			logging.warning('debdiff reported unknown return code %s %s %s:', process.returncode, debian_dsc_name, dsc_name)
			logging.warning(output)
			cmdline = ['ls', '-lR', '--time-style=+']
			for name, dir in (derivative_short_name, tmp_dir), ('Debian', debian_tmp_dir):
				logging.warning('dir listing for %s:', name)
				process = subprocess.Popen(cmdline, cwd=dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, preexec_fn=subprocess_setup)
				output = process.communicate()[0]
				logging.warning(output)
			return False

	# Filter the main patch to include only the debian/ directory
	if os.path.exists(path_everything) and not os.path.exists(path_debian):
		makedirs(os.path.dirname(path_debian))
		cmdline = ['filterdiff', '--include=*/debian/*', path_everything]
		filterdiff = subprocess.Popen(cmdline, stdout=subprocess.PIPE, preexec_fn=subprocess_setup)
		filterdiff_output = filterdiff.communicate()[0]
		diffstat = subprocess.Popen('diffstat', stdin=subprocess.PIPE, stdout=subprocess.PIPE, preexec_fn=subprocess_setup)
		diffstat_output = diffstat.communicate(filterdiff_output)[0]
		f = open(path_debian, 'w')
		f.write('diffstat of debian/ for %s %s\n' % (os.path.splitext(debian_dsc_name)[0], os.path.splitext(dsc_name)[0]))
		f.write('\n')
		f.write(diffstat_output)
		f.write('\n')
		f.write(filterdiff_output)
		f.close()

	# Patches > 100MB are probably not that useful, replace them with a link
	for path in path_everything, path_debian:
		try:
			if os.path.getsize(path) > 104857600:
				remove(path)
				symlink(os.path.relpath(patch_too_large, os.path.dirname(path)), path)
		except OSError:
			pass

	return True

def check_patch(debian_dsc_sha1, dsc_sha1):
	patch_path = sha1_patch_path(debian_dsc_sha1, dsc_sha1)
	lsdiff_path = sha1_lsdiff_path(debian_dsc_sha1, dsc_sha1)
	if os.path.exists(lsdiff_path):
		logging.debug('lsdiff cache exists for %s', patch_path)
		f = file(lsdiff_path)
		lsdiff = f.read()
		f.close()
	else:
		logging.debug('lsdiff cache does not exist for %s', patch_path)
		cmdline = ['lsdiff', patch_path]
		process = subprocess.Popen(cmdline, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, preexec_fn=subprocess_setup)
		lsdiff = process.communicate()[0]
		makedirs(os.path.dirname(lsdiff_path))
		f = file(lsdiff_path,'w')
		f.write(lsdiff)
		f.close()
	lsdiff = lsdiff.splitlines()
	for line in lsdiff:
		if line != 'debian/changelog' and not line.endswith('/debian/changelog'):
			return True
	return False

def present_patch(name, version, dsc_sha1, debian_name, debian_version,  debian_dsc_sha1):
	useful_patch = check_patch(debian_dsc_sha1, dsc_sha1)
	patches = []
	types = ('', 'debian')
	for type in types:
		ln_to = sha1_patch_path(debian_dsc_sha1, dsc_sha1, type)
		if not os.path.exists(ln_to):
			continue
		ln_from_deriv = deriv_patch_path(name, version, debian_name, debian_version, type)
		ln_from_global = global_patch_path(name, version, debian_name, debian_version, type)
		makedirs(os.path.dirname(ln_from_deriv))
		makedirs(os.path.dirname(ln_from_global))
		remove(ln_from_deriv)
		remove(ln_from_global)
		symlink(os.path.relpath(ln_to, os.path.dirname(ln_from_deriv)), ln_from_deriv)
		symlink(os.path.relpath(ln_to, os.path.dirname(ln_from_global)), ln_from_global)
		if useful_patch:
			patches.append(os.path.relpath(ln_from_global, os.path.abspath(global_patch_dir)))
	return tuple(patches)

# Functions that wrap other functions and decide what to do

def check_source_package(source_entry, srcpkg):
	global modifies_dsc_files

	try:
		name = None
		version = None
		dir = None
		name = srcpkg['Package']
		version = srcpkg['Version']
		dir = srcpkg['Directory']
		if '/' in name or name == '..':
			logging.warning('could not process source package %s %s: possibly malicious name', name, version)
			return None
		if '/' in version or version == '..':
			logging.warning('could not process source package %s %s: possibly malicious version', name, version)
			return None
		if '..' in dir.split('/'):
			logging.warning('could not process source package %s %s: possibly malicious dir: %s', name, version, dir)
			return None
	except KeyError:
		logging.warning('could not process source package %s %s', name, version)
		return None
	logging.debug('started processing source package %s %s', name, version)
	info = get_info(srcpkg)
	if not info:
		logging.warning('finished processing source package %s %s: could not get any info', name, version)
		return None
	dsc_hash_type, dsc_hash, dsc_name, parts = info
	if '/' in dsc_name or dsc_name == '..':
		logging.warning('could not process source package %s %s: possibly malicious dsc name %s', name, version, dsc_name)
		return None
	if not ishex(dsc_hash):
		logging.warning('could not process source package %s %s: possibly malicious dsc hash %s', name, version, dsc_hash)
		return None
	dsc_url = source_entry.archive_uri('%s/%s' % (dir, dsc_name))
	logging.debug('found dsc file: %s %s %s', dsc_hash_type, dsc_hash, dsc_url)
	dsc_status, dsc_sha1 = status(dsc_hash_type, dsc_hash, dsc_url)
	logging.debug('checked dsc status: %s %s %s', dsc_status, dsc_sha1, dsc_url)
	if dsc_status == 'unmodified':
		# Ignore the srcpkg since we know it is was in Debian
		# at one point and is hopefully therefore unmodified
		logging.debug('finished processing source package %s %s: dsc unmodified', name, version)
		return None
	else:
		files = [(dsc_sha1, dsc_hash_type, dsc_hash)]
		parts_unmodified = []
		parts_modified = []
		parts_unknown = []
		for part in parts:
			part_name = part['name']
			if '/' in part_name or part_name == '..':
				logging.warning('could not process source package %s %s: possibly malicious part name %s', name, version, part_name)
				return None
			part_url = source_entry.archive_uri('%s/%s' % (dir, part_name))
			part_hash_type, part_hash = [(k, v) for k, v in part.iteritems() if k not in ('name', 'size')][0]
			if not ishex(part_hash):
				logging.warning('could not process source package %s %s: possibly malicious part hash %s', name, version, part_hash)
				return None
			logging.debug('found part file: %s %s %s', part_hash_type, part_hash, part_url)
			part_status, part_sha1 = status(part_hash_type, part_hash, part_url)
			logging.debug('checked part status: %s %s %s', part_status, part_sha1, part_url)
			if 'sha1' not in part and part_sha1: part['sha1'] = part_sha1
			if part_status == 'unmodified': parts_unmodified.append((part_sha1, part_name))
			elif part_status == 'modified': parts_modified.append((part_sha1, part_name))
			else: parts_unknown.append((part_sha1, part_name))
			if part_status == 'modified': files.append((part_sha1, part_hash_type, part_hash))

		all_parts_unmodified = (len(parts_unmodified) == len(parts))
		all_parts_modified = (len(parts_modified) == len(parts))
		parts_unmodified = list(set(parts_unmodified))
		logging.debug('source package status %s %s: dsc %s, %s parts unmodified, %s parts modified, %s parts unknown', name, version, dsc_status, len(parts_unmodified), len(parts_modified), len(parts_unknown))

		if all_parts_unmodified:
			# Ignore the srcpkg since we know all the parts were
			# in Debian at one point and ergo, it is unmodified
			logging.debug('finished processing source package %s %s: all non-dsc parts unmodified', name, version)
			if dsc_status == 'modified':
				logging.info('source package %s %s: unmodified, but dsc different', name, version)
				modifies_dsc_files += 1
			return (files, None, None, None)
		else:
			logging.debug('some parts modified, looking for derived version %s %s', name, version)
			if not dsc_sha1:
				logging.warning('finished processing source package %s %s: sha1 missing for dsc file', name, version)
				return (files, None, None, None)
			if parts_unknown:
				logging.warning('finished processing source package %s %s: sha1 missing for some parts', name, version)
				return (files, None, None, None)
			new = None
			link = None
			patch = None
			tmp_dir = prepare(dsc_name, dsc_sha1, parts)
			derived_from = find_derived_from(tmp_dir, name, version, dsc_name, dsc_sha1, parts_unmodified)
			if derived_from:
				debian_name, debian_version = derived_from
				link = (debian_name, debian_version, name, version, dsc_url)
				logging.debug('source package %s %s derived from %s %s', name, version, debian_name, debian_version)
				debian_files = srcpkg_to_files(debian_name, debian_version)
				if debian_files:
					debian_info = get_debian_info(debian_files)
					if debian_info:
						debian_dsc_sha1, debian_dsc_name, debian_parts = debian_info
						logging.debug('Debian source package %s %s dsc found %s %s', debian_name, debian_version, debian_dsc_name, debian_dsc_sha1)
						debian_tmp_dir = prepare_debian(debian_dsc_name, debian_dsc_sha1, debian_parts)
						patch_created = create_patch(tmp_dir, dsc_name, dsc_sha1, debian_tmp_dir, debian_dsc_name, debian_dsc_sha1)
						patch_names = present_patch(name, version, dsc_sha1, debian_name, debian_version, debian_dsc_sha1)
						if patch_names: patch = (debian_name, debian_version, debian_dsc_sha1, name, version, dsc_sha1, [part[0] for part in parts_modified], patch_names)
						rmtree(debian_tmp_dir)
					else:
						logging.warning('source package %s %s: could not get Debian info for %s %s: %s', name, version, debian_name, debian_version, debian_info)
				else:
					if srcpkg_was_in_debian(debian_name, debian_version):
						logging.warning('source package %s %s: snapshot database issue, no Debian files found', debian_name, debian_version)
					else:
						logging.warning('source package %s %s: derived from %s %s possibly bogus', name, version, debian_name, debian_version)
			else:
				new = (name, version, dsc_url)
			rmtree(tmp_dir)
			logging.debug('finished processing source package %s %s: all done', name, version)
			return (files, patch, link, new)

def process_sources(source_entries, lists_dir):
	files = []
	patches = []
	links = []
	new = []
	for source in source_entries:
		for source_entry in source:
			fn = os.path.join(lists_dir, source_entry.describe.rstrip(')').rpartition('(')[2])
			try: f = file(fn)
			except IOError: continue
			for srcpkg in deb822.Sources.iter_paragraphs(f):
				actions = check_source_package(source_entry, srcpkg)
				if actions:
					action_files, action_patch, action_link, action_new = actions
					if action_files:
						files.append(action_files)
						logging.debug('action: return files %s', ' '.join([' '.join(action) for action in action_files]))
					if action_patch:
						patches.append(action_patch)
						logging.debug('action: return patches %s', ' '.join([' '.join(action) for action in action_patch]))
					if action_link:
						links.append(action_link)
						logging.debug('action: return links to modified source packages %s', ' '.join(action_link))
					if action_new:
						new.append(action_new)
						logging.debug('action: return links to new source packages %s', ' '.join(action_new))
				logging.debug('done')
				logging.debug('')
			f.close()
	return (files, patches, links, new)

logging.debug('processing distribution %s', derivative_short_name)

files, patches, links, new = process_sources(source_entries, lists_dir)

# Done with the database, close the connection
cur.close()
conn.close()

# Write out the results
filename = sys.argv[3]
data = files
if data:
	output_data = {}
	for package in data:
		for modified_file in package:
			sha1, hash_type, hash = modified_file
			if sha1 not in output_data:
				output_data[sha1] = {}
			if hash_type != 'sha1' and hash_type not in output_data[sha1]:
				output_data[sha1][hash_type] = hash
			elif hash_type != 'sha1' and hash != output_data[sha1][hash_type]:
				logging.warning('hashes mismatched: %s: %s %s != %s', sha1, hash_type, hash, output_data[sha1][hash_type])
	output = file(os.path.abspath(filename), 'wb')
	yaml.safe_dump(output_data, output)
	output.close()

filename = sys.argv[4]
data = patches
if data:
	if not os.path.exists(os.path.join(global_patch_dir,'HEADER.html')):
		symlink('../../doc/HEADER.patches.html',os.path.join(global_patch_dir,'HEADER.html'))
	if not os.path.exists(os.path.join(global_patch_dir,'.htaccess')):
		symlink('../../etc/htaccess.patches',os.path.join(global_patch_dir,'.htaccess'))
	if not os.path.exists(os.path.join(deriv_patch_dir,'HEADER.html')):
		symlink('../../../doc/HEADER.patches.html',os.path.join(deriv_patch_dir,'HEADER.html'))
	if not os.path.exists(os.path.join(global_patch_dir,'.htaccess')):
		symlink('../../../etc/htaccess.patches',os.path.join(deriv_patch_dir,'.htaccess'))
	output_data = []
	for item in data:
		debian_name, debian_version, debian_sha1, name, version, sha1, parts_sha1, patches = item
		item = {}
		item['debian_name'] = debian_name
		item['debian_version'] = debian_version
		item['debian_sha1'] = debian_sha1
		item['name'] = name
		item['version'] = version
		item['sha1'] = sha1
		item['patches'] = patches
		item['parts'] = parts_sha1
		output_data.append(item)
	output = file(os.path.abspath(filename), 'wb')
	yaml.safe_dump(output_data, output)
	output.close()
else:
	remove(filename)

filename = sys.argv[5]
data = links
if data:
	data = list(set(data))
	data.sort(cmp=lambda a,b: cmp(a[0],b[0]) or apt_pkg.version_compare(a[1],b[1]) or cmp(a[2],b[2]) or apt_pkg.version_compare(a[3],b[3]))
	output_data = {}
	output = file(os.path.abspath(filename), 'wb')
	for debian_name, debian_version, name, version, dsc_url in data:
		if debian_name not in output_data:
			output_data[debian_name] = {}
		if debian_version not in output_data[debian_name]:
			output_data[debian_name][debian_version] = []
		item = {}
		item['name'] = name
		item['version'] = version
		item['dsc'] = dsc_url
		output_data[debian_name][debian_version].append(item)
	yaml.safe_dump(output_data, output)
	output.close()
else:
	remove(filename)

filename = sys.argv[6]
data = new
if data:
	data = list(set(data))
	data.sort(cmp=lambda a,b: cmp(a[0],b[0]) or apt_pkg.version_compare(a[1],b[1]) or cmp(a[2],b[2]))
	output_data = {}
	output = file(os.path.abspath(filename), 'wb')
	for name, version, dsc_url in data:
		if name not in output_data:
			output_data[name] = {}
		if version not in output_data[name]:
			output_data[name][version] = []
		output_data[name][version].append(str(dsc_url))
	yaml.safe_dump(output_data, output)
	output.close()
else:
	remove(filename)

logging.shutdown()

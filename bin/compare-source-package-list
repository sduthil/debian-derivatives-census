#!/usr/bin/python

# Copyright 2011 Paul Wise
# Released under the MIT/Expat license, see doc/COPYING

# Uses the snapshot.debian.org metadata database and SHA-1 based filesystem to
# compute debdiffs between Debian and individual derivatives. The metadata
# allows knowing if a particular file was ever in Debian and the filesystem
# allows the creation of debdiffs.
#
# The script works approximately like this:
#
# Load the Sources files previously downloaded by get-package-lists as indicated
# by the sources.list of the derivative.
#
# For each source package in the derivative:
#
# Check if the dsc has ever been in Debian, if not, check if the other
# parts have and therefore decide if the package is unmodified or not.
# Unmodified source packages are skipped and include those with the exact
# same dsc file or those where all the non-dsc parts are identical.
#
# Try some heuristics (name, version, changelog entries) to find out if
# the package could be based on some package that is or was in Debian.
#
# If it was not then skip to the next one and make a note, since Debian
# might want to know about source packages that are missing from Debian.
#
# If it was then use debdiff to create a diff and filterdiff to create a
# diff of the debian/ dir.
#
# Usage:
# compare-source-package-list <sources.list> <apt dir> <patches list> <links list> <new package list> <log file>

# FIXME: write out some statistics and rrdtool graphs
#		source package types per derivative
#		number of source packages
#		cache misses: md5, sha256, sha1, patch, changelog
# FIXME: comment the code to list assumptions and function purpose
# FIXME: add options to allow re-processing only specific packages
# FIXME: write something to clean up old files and patches
# FIXME: don't unpack or make a patch when we don't have all the parts
# FIXME: don't make a patch when we were not able to unpack the source package
# FIXME: cleanup files at start of run
# FIXME: extract new debian/patches/ patches
# FIXME: print out packages that are no longer in Debian
# FIXME: deal with really large patches:
# FIXME:   kde-l10n-*: too few parts to be useful
# FIXME:   divergence: too many changelog entries between versions to be useful
# FIXME:   derivative is older than Debian
# FIXME:   derivative renamed the source package
# FIXME:   just a really big diff
# FIXME: when there are multiple dsc files in snapshots, prefer the debian/debian-archive one
# FIXME: when the source package is ancient and the dsc is missing, make a fake one to use
# FIXME: add an in-memory cache of hashes so that hashes in multiple releases hit the disk once
# FIXME: deal with rate-limited websites like alioth that do not like many requests

import re
import os
import sys
import requests
import hashlib
import shutil
import logging
import tempfile
import string
import socket
import signal
import subprocess
import yaml
from debian import deb822, changelog
import apt_pkg
import psycopg2
try: import simplejson as json
except ImportError: import json
import struct

# Helper functions for python stuff with annoying error handling

def makedirs(dirs):
	try: os.makedirs(dirs)
	except OSError: pass

def rmtree(dir):
	try: shutil.rmtree(dir)
	except OSError: pass

def remove(file):
	try: os.remove(file)
	except OSError: pass

def symlink(source, link):
	try: os.symlink(source, link)
	except OSError: pass

# http://www.chiark.greenend.org.uk/ucgi/~cjwatson/blosxom/2009-07-02-python-sigpipe.html
def subprocess_setup():
	# Python installs a SIGPIPE handler by default. This is usually not what
	# non-Python subprocesses expect.
	signal.signal(signal.SIGPIPE, signal.SIG_DFL)

# We need to map apt_pkg.version_compare return values to cmp return values
# The documentation is incorrect: https://bugs.debian.org/680891
def apt_version_cmp(a, b):
	ret = apt_pkg.version_compare(a, b)
	if ret < 0: return -1
	elif ret > 0: return 1
	else: return 0

# Config
md5_cache_dir = os.path.abspath('../md5-farm')
sha1_cache_dir = os.path.abspath('../sha1-farm')
sha256_cache_dir = os.path.abspath('../sha256-farm')
sha1_patch_dir = os.path.abspath('../sha1-patches')
sha1_lsdiff_dir = os.path.abspath('../sha1-lsdiff')
sha1_changelog_dir = os.path.abspath('../sha1-changelog')
deriv_patch_dir = os.path.abspath('patches')
global_patch_dir = os.path.abspath('../patches')
snapshot_cache_dir = '/srv/snapshot.debian.org/farm'
patch_too_large = os.path.abspath('../../doc/patch-too-large.txt')
checksum_types = ('sha1', 'sha256', 'md5sum')
checksum_hashlib = ('sha1', 'sha256', 'md5')
checksum_headers = ('Checksums-Sha1', 'Checksums-Sha256', 'Files')
user_agent = 'Debian Derivatives Census QA bot'
timeout = 60
ishex = lambda s: not(set(s)-set(string.hexdigits))
debian_ssl_bundle = '/etc/ssl/ca-global/ca-certificates.crt'
if os.path.exists(debian_ssl_bundle):
	ssl_verify = debian_ssl_bundle
else:
	ssl_verify = True

# Init
apt_pkg.init()

# Preparation
sources_list = apt_pkg.SourceList()
sources_list.read_main_list()
conn = psycopg2.connect("service=snapshot-guest")
cur = conn.cursor()
remove(sys.argv[7])
logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG, filename=sys.argv[7])

# Voodoo
lists_dir = apt_pkg.config.find_dir('Dir::State::lists')
source_entries = [[i for i in x.index_files if i.label=='Debian Source Index'] for x in sources_list.list]
derivative_short_name = os.path.basename(os.getcwd())
modifies_dsc_files = 0
repackaged_but_identical = 0

# Generic helper functions

def uncompressed_size(filename):
	uc_size = 0
	file_size = os.path.getsize(filename)
	with open(filename, 'rb') as f:
		magic = f.read(6)
		# *.gz
		if magic[:2] == "\x1f\x8b":
			f.seek(-4, 2)
			data = f.read()
			uc_size = struct.unpack('<I', data)[0]
		# *.bz2
		elif magic[:3] == 'BZh':
			# Crude estimate based on average compression ratio of 25%
			uc_size = file_size*4
		# *.xz
		elif magic == "\xfd7zXZ\x00":
			cmdline = ['xz', '--verbose', '--list', filename]
			process = subprocess.Popen(cmdline, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, preexec_fn=subprocess_setup)
			output = process.communicate()[0]
			if process.returncode:
				logging.warning('xz reported failure to check size of %s:', filename)
				logging.warning(output)
			else:
				for line in output.splitlines():
					line = line.strip()
					if line.startswith('Uncompressed size:'):
						match = re.match(r'Uncompressed size:  .*?([0-9,]+) B', line)
						if match: uc_size = int(''.join(match.group(1).split(',')))
						else: logging.warning('xz reported weird output for %s: %s', filename, line)
		# *.lz
		elif magic[:4] == 'LZIP':
			f.seek(-16, 2)
			data = f.read(8)
			uc_size = struct.unpack('<Q', data)[0]
	return max(file_size, uc_size)

def tmp_size():
	stats = os.statvfs(tempfile.gettempdir())
	return stats.f_frsize*stats.f_blocks

def tmp_space():
	stats = os.statvfs(tempfile.gettempdir())
	return stats.f_frsize*stats.f_bavail

def tmp_environ(tmp_dir):
	tmp_env = { 'TMP': tmp_dir, 'TMPDIR': tmp_dir, 'TEMP': tmp_dir, 'TEMPDIR': tmp_dir }
	return dict(list(os.environ.items()) + list(tmp_env.items()))

# Helper functions for generating path names

def hash_path_parent(dir, hash):
	return os.path.join(dir, hash[0:2], hash[2:4])

def hash_path(dir, hash):
	return os.path.join(dir, hash[0:2], hash[2:4], hash)

def hash_path_exists(dir, hash):
	return os.path.exists(os.path.join(dir, hash[0:2], hash[2:4], hash))

def snapshot_hash_path(hash):
	return hash_path(snapshot_cache_dir, hash)

def snapshot_hash_path_exists(hash):
	return hash_path_exists(snapshot_cache_dir, hash)

def part_hash_path(part):
	if 'sha1' in part:
		path = snapshot_hash_path(part['sha1'])
		if not os.path.exists(path): path = hash_path(sha1_cache_dir, part['sha1'])
		return path
	elif 'sha256' in part:
		return hash_path(sha256_cache_dir, part['sha256'])
	elif 'md5sum' in part:
		return hash_path(md5_cache_dir, part['md5sum'])
	else:
		return None

def sha1_patch_path(debian_dsc_sha1, dsc_sha1, type=None):
	path = os.path.join(hash_path(sha1_patch_dir, debian_dsc_sha1), hash_path('', dsc_sha1))
	if type: path += '.%s' % type
	path += '.patch'
	return os.path.abspath(path)

def sha1_lsdiff_path(debian_dsc_sha1, dsc_sha1, type=None):
	path = os.path.join(hash_path(sha1_lsdiff_dir, debian_dsc_sha1), hash_path('', dsc_sha1))
	if type: path += '.%s' % type
	path += '.lsdiff'
	return os.path.abspath(path)

def shortslug(name):
	return name[:4] if name.startswith('lib') else name[0]

def deriv_patch_path(name, version, debian_name, debian_version, type=None):
	path = os.path.join(deriv_patch_dir, shortslug(debian_name), debian_name, '')
	path += '_'.join((debian_name, debian_version, name, version))
	if type: path += '.%s' % type
	path += '.patch'
	return os.path.abspath(path)

def global_patch_path(name, version, debian_name, debian_version, type=None):
	path = os.path.join(global_patch_dir, shortslug(debian_name), debian_name, '')
	path += '_'.join(('Debian', debian_name, debian_version, derivative_short_name, name, version))
	if type: path += '.%s' % type
	path += '.patch'
	return os.path.abspath(path)

# Functions for munging source packages

def convert_lzip_to_gzip(dir, name):
	cmdline = ['lzip', '-d', '--', name]
	process = subprocess.Popen(cmdline, cwd=dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, preexec_fn=subprocess_setup)
	output = process.communicate()[0]
	if process.returncode:
		logging.warning('lzip reported failure to decompress %s:', name)
		logging.warning(output)
		return None
	bname = name[0:-3] # Strip off .lz
	cmdline = ['gzip', '-1', '--', bname] # gzip -1 to reduce overhead
	process = subprocess.Popen(cmdline, cwd=dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, preexec_fn=subprocess_setup)
	output = process.communicate()[0]
	if process.returncode:
		logging.warning('gzip reported failure to compress %s:', bname)
		logging.warning(output)
		return None
	return (name, bname+'.gz')

def update_dsc_file(dir, dsc_name, parts):
	dsc_path = os.path.join(dir,dsc_name)
	dsc_file = open(dsc_path,'rb')
	dsc = deb822.Dsc(dsc_file)
	for (old, name) in parts:
		path = os.path.join(dir,name)
		size = os.path.getsize(path)
		with open(path,'rb') as f:
			hashes = {}
			for (type, func) in zip(checksum_types, checksum_hashlib):
				hashes[type] = getattr(hashlib, func)()
			for chunk in iter(lambda: f.read(128*64L), b''):
				for type in checksum_types:
					hashes[type].update(chunk)
			for type in checksum_types:
				hashes[type] = hashes[type].hexdigest()
			for (header, type) in zip(checksum_headers, checksum_types):
				if header in dsc:
					dsc[header] = [{type: hashes[type], 'size': size, 'name': name} if p['name'] == old else p for p in dsc[header]]
	dsc_file.close()
	os.remove(dsc_path) # So we don't change the original that the dsc links to
	with open(dsc_path,'wb') as dsc_file:
		dsc.dump(dsc_file)

# Functions for downloading files and storing them in the hash caches

def download_and_check_hash(url, dir, hash, hash_type):
	try:
		parent = hash_path_parent(dir,hash)
		path = hash_path(dir,hash)
		logging.debug('downloading %s', url)
		makedirs(parent)
		headers = { 'User-Agent' : user_agent }
		req = requests.get(url, headers=headers, timeout=timeout, verify=ssl_verify)
		req.raise_for_status() # to catch HTTP errors
		data = req.content
		if hash_type == 'sha256':
			data_hash = hashlib.sha256(data).hexdigest()
		elif hash_type == 'md5sum':
			data_hash = hashlib.md5(data).hexdigest()
		else:
			logging.warning('unknown hash type detected: %s %s %s', hash_type, hash, url)
			return ('unknown', None)
		if data_hash != hash:
			logging.warning('incorrect hash for downloaded file, ignoring: %s %s != %s %s', hash_type, hash, data_hash, url)
			return ('unknown', None)
		sha1 = hashlib.sha1(data).hexdigest()
		sha1_path = hash_path(sha1_cache_dir, sha1)
		sha1_parent = hash_path_parent(sha1_cache_dir, sha1)
		makedirs(sha1_parent)
		snapshot_path = snapshot_hash_path(sha1)
		if os.path.exists(snapshot_path):
			symlink(snapshot_path, path)
			logging.debug('exists in snapshot sha1 cache: %s %s %s %s', hash_type, hash, sha1, url)
			return (True, sha1)
		else:
			if not os.path.exists(sha1_path):
				logging.debug('correct hash for downloaded file, saving: %s %s %s %s', hash_type, hash, sha1, url)
				with open(sha1_path, 'w') as f:
					f.write(data)
			else:
				logging.debug('correct hash for downloaded file, not saving: already in derivs cache: %s %s %s %s', hash_type, hash, sha1, url)
			symlink(os.path.relpath(sha1_path, os.path.dirname(path)), path)
			logging.debug('does not exist in snapshot sha1 cache: %s %s %s %s', hash_type, hash, sha1, url)
			return (False, sha1)
	except requests.ConnectionError as e:
		logging.warning('unable to download hash file, ignoring: %s %s', e, url)
		return ('unknown', None)
	except requests.HTTPError, e:
		st = req.status_code
		logging.warning('unable to download hash file, ignoring: %s: %s %s', st, e, url)
		return ('unknown', None)
	except socket.error, e:
		logging.warning('unable to download hash file, ignoring: %s %s', e, url)
		return ('unknown', None)

def download_sha1(url, dir, sha1):
	try:
		parent = hash_path_parent(dir,sha1)
		path = hash_path(dir,sha1)
		logging.debug('downloading sha1: %s %s', sha1, url)
		makedirs(parent)
		headers = { 'User-Agent' : user_agent }
		req = requests.get(url, headers=headers, timeout=timeout, verify=ssl_verify)
		req.raise_for_status() # to catch HTTP errors
		data = req.content
		data_sha1 = hashlib.sha1(data).hexdigest()
		if data_sha1 == sha1:
			logging.debug('correct sha1 for downloaded file, saving: %s %s', sha1, url)
			if not os.path.exists(path):
				with open(path, 'w') as f:
					f.write(data)
			return (False, sha1)
		else:
			logging.warning('incorrect sha1 for downloaded file, ignoring: %s != %s %s', sha1, data_sha1, url)
			return ('unknown', None)
	except requests.ConnectionError as e:
		logging.warning('unable to download sha1 file, ignoring: %s %s', e, url)
		return ('unknown', None)
	except requests.HTTPError as e:
		st = req.status_code
		logging.warning('unable to download sha1 file, ignoring: %s: %s %s', st, e, url)
		return ('unknown', None)
	except socket.error, e:
		logging.warning('unable to download sha1 file, ignoring: %s %s', e, url)
		return ('unknown', None)

# Functions for checking the hash caches

def check_hash_cache(dir, hash, hash_type, url):
	logging.debug('checking hash cache: %s %s', hash_type, hash)
	path = hash_path(dir, hash)
	try:
		result = os.readlink(path)
		path = os.path.join(os.path.dirname(path), result)
	except OSError:
		logging.debug('does not exist in hash cache: %s %s', hash_type, hash)
		return download_and_check_hash(url, dir, hash, hash_type)
	logging.debug('exists in hash cache: %s %s', hash_type, hash)
	sha1 = os.path.basename(path)
	if snapshot_hash_path_exists(sha1):
		logging.debug('exists in snapshot sha1 cache: %s', sha1)
		remove(hash_path(sha1_cache_dir,sha1))
		return (True, sha1)
	elif hash_path_exists(sha1_cache_dir, sha1):
		logging.debug('exists in derivatives sha1 cache: %s', sha1)
		return (False, sha1)
	else:
		logging.debug('missing in derivatives sha1 cache: %s', sha1)
		return download_and_check_hash(url, dir, hash, hash_type)

def check_sha1_cache(sha1, url):
	logging.debug('checking sha1 caches: %s', sha1)
	if snapshot_hash_path_exists(sha1):
		logging.debug('exists in snapshot sha1 cache: %s', sha1)
		remove(hash_path(sha1_cache_dir,sha1))
		return (True, sha1)
	elif hash_path_exists(sha1_cache_dir, sha1):
		logging.debug('exists in derivatives sha1 cache: %s', sha1)
		return (False, sha1)
	else:
		logging.debug('does not exist in any sha1 caches: %s', sha1)
		return download_sha1(url, sha1_cache_dir, sha1)

def status(type, hash, url):
	logging.debug('checking status of hash: %s %s %s', type, hash, url)
	if type == 'sha1':
		(ret, sha1) = check_sha1_cache(hash, url)
		if ret == True:
			return ('unmodified', sha1)
		elif ret == False:
			return ('modified', sha1)
		else:
			return (ret, sha1)
	elif type == 'sha256':
		(ret, sha1) = check_hash_cache(sha256_cache_dir, hash, type, url)
		if ret == True:
			return ('unmodified', sha1)
		elif ret == False:
			return ('modified', sha1)
		else:
			return (ret, sha1)
	elif type == 'md5sum':
		(ret, sha1) = check_hash_cache(md5_cache_dir, hash, type, url)
		if ret == True:
			return ('unmodified', sha1)
		elif ret == False:
			return ('modified', sha1)
		else:
			return (ret, sha1)
	else:
		logging.warning('unknown hash type detected: %s %s %s', type, hash, url)
		return ('unknown', None)

# Functions for getting information about source packages

def get_info(srcpkg):
	dsc = None
	for header in checksum_headers:
		if not dsc and header in srcpkg:
			dsc = [x for x in srcpkg[header] if x['name'].endswith('.dsc')]
	if not dsc:
		logging.warning('did not find any dsc files')
		return None
	if len(dsc) > 1:
		logging.warning('found multiple dsc files: %s' % ' '.join(['%s %s' % (dsc_name, dsc_sha1) for dsc_name, dsc_sha1 in dsc]))
		return None
	dsc = dsc[0]
	dsc_name = dsc['name']
	dsc_hash_type, dsc_hash =  [(k, v) for k, v in dsc.iteritems() if k not in ('name', 'size')][0]

	parts = []
	part_names = []
	for header in checksum_headers:
		if header in srcpkg:
			for part in srcpkg[header]:
				if 'name' in part and part['name'] not in part_names and not part['name'].endswith('.dsc'):
					parts.append(part)
					part_names.append(part['name'])

	return (dsc_hash_type, dsc_hash, dsc_name, parts)

def get_debian_info(files):
	dsc = [file for file in files if file[0].endswith('.dsc')]
	if not dsc:
		logging.warning('did not find any Debian dsc files: snapshots bug or ancient source package')
		return None
	if len(dsc) > 1:
		logging.warning('found multiple Debian dsc files, choosing first one: %s' % ' '.join(['%s %s' % (dsc_name, dsc_sha1) for dsc_name, dsc_sha1 in dsc]))

	dsc = dsc[0]
	dsc_name, dsc_sha1 = dsc

	parts = []
	part_names = []
	for file in files:
		part_name, part_sha1 = file
		if part_name not in part_names and not part_name.endswith('.dsc'):
			parts.append(file)
			part_names.append(part_name)

	return (dsc_sha1, dsc_name, parts)

# Functions for extracting information from the snapshots database

def database_error(e):
	reason = None
	code = None
	if hasattr(e, 'pgerror'): reason = e.pgerror
	if hasattr(e, 'pgcode'): code = e.pgcode
	logging.warning('unable to execute database query: %s %s', code, reason)
	conn.reset()

def srcpkg_was_in_debian(name, version=None):
	try:
		if version:
			cur.execute('SELECT version FROM srcpkg WHERE name=%s AND version=%s LIMIT 1;', (name, version))
			return not not cur.fetchone()
		else:
			cur.execute('SELECT version FROM srcpkg WHERE name=%s LIMIT 1;', (name,))
			return not not cur.fetchone()
	except psycopg2.Error, e:
		database_error(e)
		return None

def sha1_to_srcpkgs(sha1):
	try:
		cur.execute(
			'''SELECT name, version
			FROM srcpkg
			JOIN file_srcpkg_mapping ON file_srcpkg_mapping.srcpkg_id=srcpkg.srcpkg_id
			WHERE hash=%s;''', (sha1,))
		return cur.fetchall()
	except psycopg2.Error, e:
		database_error(e)
		return None

def srcpkg_to_sha1s(name, version):
	try:
		cur.execute(
			'''SELECT hash
			FROM file_srcpkg_mapping
			JOIN srcpkg ON srcpkg.srcpkg_id=file_srcpkg_mapping.srcpkg_id
			WHERE name=%s AND version=%s;''', (name, version))
		return cur.fetchall()
	except psycopg2.Error, e:
		database_error(e)
		return None

def srcpkg_to_srcpkgs(name):
	try:
		cur.execute(
			'''SELECT name, version
			FROM srcpkg
			WHERE name=%s ORDER BY version DESC;''', (name,))
		return cur.fetchall()
	except psycopg2.Error, e:
		database_error(e)
		return None

def sha1s_to_files(sha1):
	try:
		cur.execute('SELECT DISTINCT ON (name, hash) name, hash FROM file WHERE hash=%s;', hash)
		return cur.fetchall()
	except psycopg2.Error, e:
		database_error(e)
		return None

def srcpkg_to_files(name, version):
	try:
		cur.execute(
			'''SELECT DISTINCT ON (file.name, file.hash) file.name, file.hash
			FROM file_srcpkg_mapping
			JOIN srcpkg ON srcpkg.srcpkg_id=file_srcpkg_mapping.srcpkg_id
			JOIN file ON file_srcpkg_mapping.hash=file.hash
			WHERE srcpkg.name=%s AND srcpkg.version=%s;''', (name, version))
		return cur.fetchall()
	except psycopg2.Error, e:
		database_error(e)
		return None

def sha1_version_to_derived_from(sha1, version):
	try:
		cur.execute(
			'''SELECT name, version
			FROM srcpkg
			JOIN file_srcpkg_mapping ON file_srcpkg_mapping.srcpkg_id=srcpkg.srcpkg_id
			WHERE hash=%s and version<=%s
			ORDER BY name ASC, version DESC
			LIMIT 1;''', (sha1, version))
		res = cur.fetchall()
		if res: return res
		cur.execute(
			'''SELECT name, version
			FROM srcpkg
			JOIN file_srcpkg_mapping ON file_srcpkg_mapping.srcpkg_id=srcpkg.srcpkg_id
			WHERE hash=%s
			ORDER BY name ASC, version ASC
			LIMIT 1;''', (sha1, version))
		return cur.fetchall()
	except psycopg2.Error, e:
		database_error(e)
		return None

def srcpkg_to_derived_from(name, version):
	try:
		cur.execute(
			'''SELECT name, version
			FROM srcpkg
			WHERE name=%s and version<=%s
			ORDER BY version DESC
			LIMIT 1;''', (name, version))
		res = cur.fetchall()
		if res: return res
		cur.execute(
			'''SELECT name, version
			FROM srcpkg
			WHERE name=%s
			ORDER BY version ASC
			LIMIT 1;''', (name,))
		return cur.fetchall()
	except psycopg2.Error, e:
		database_error(e)
		return None

# Functions related to creating patches

# Add symlinks for all needed files
def prepare(dsc_name, dsc_sha1, parts):
	logging.debug('preparing deriv directory for %s', dsc_name)
	total_size = 0
	unreadable_parts = []
	symlink_parts = []
	convert_lzip_parts = []
	converted_parts = []
	path = snapshot_hash_path(dsc_sha1)
	if not os.path.exists(path): path = hash_path(sha1_cache_dir, dsc_sha1)
	if not os.access(path, os.R_OK): unreadable_parts.append(dsc_name)
	symlink_parts.append((path, dsc_name))
	for part in parts:
		path = part_hash_path(part)
		if not path: continue
		if not os.access(path, os.R_OK):
			unreadable_parts.append(part['name'])
			continue
		size = uncompressed_size(path)
		total_size += size
		symlink_parts.append((path, part['name']))
		if part['name'].endswith('.lz'):
			total_size += size
			convert_lzip_parts.append(part['name'])

	if unreadable_parts:
		logging.warning('some parts of %s are unreadable: %s', dsc_name, ' '.join(unreadable_parts))
		return (None, total_size)

	# Ensure that the debdiff will have enough space
	tmp_parent = None
	if total_size*2.0 > tmp_size()/2.0 or total_size > tmp_space():
		logging.info('prepare: not enough space in /tmp, using scratch space: %s %s %s', total_size, tmp_size(), tmp_space())
		tmp_parent = os.path.expanduser('~/tmp')
	elif dsc_name.startswith('libreoffice_') or dsc_name.startswith('iceweasel_'):
		logging.info('prepare: iceweasel/libreoffice, always using scratch space')
		tmp_parent = os.path.expanduser('~/tmp')
	else:
		logging.debug('prepare: enough space in /tmp, not using scratch space: %s %s %s', total_size, tmp_size(), tmp_space())

	tmp_dir = tempfile.mkdtemp(prefix='derivs-cmp-srcpkg-%s-' % derivative_short_name, dir=tmp_parent)

	# Setup the symlinks for dpkg-source/debdiff/etc
	for symlink_target, symlink_name in symlink_parts:
		symlink_path = os.path.join(tmp_dir, symlink_name)
		os.symlink(symlink_target, symlink_path)

	# Some distributions allow additional compression schemes
	# Here we work around this by recompressing with gzip
	for part_name in convert_lzip_parts:
		converted = convert_lzip_to_gzip(tmp_dir, part_name)
		if converted is not None:
			converted_parts.append(converted)
		else:
			rmtree(tmp_dir)
			return (None, total_size)

	# Update the dsc file if we recompressed any files
	if converted_parts:
		update_dsc_file(tmp_dir, dsc_name, converted_parts)

	return (tmp_dir, total_size)

def prepare_debian(dsc_name, dsc_sha1, files, total_size):
	logging.debug('preparing Debian directory for %s', dsc_name)
	unreadable_parts = []
	symlink_parts = []
	path = snapshot_hash_path(dsc_sha1)
	if not os.access(path, os.R_OK): unreadable_parts.append(dsc_name)
	symlink_parts.append((path, dsc_name))
	for file in files:
		part_name, part_sha1 = file
		path = snapshot_hash_path(part_sha1)
		if not os.access(path, os.R_OK):
			unreadable_parts.append(part_name)
			continue
		size = uncompressed_size(path)
		total_size += size
		symlink_parts.append((path, part_name))

	if unreadable_parts:
		logging.warning('some parts of %s are unreadable: %s', dsc_name, ' '.join(unreadable_parts))
		return None

	# Ensure that the debdiff will have enough space
	tmp_parent = None
	if total_size > tmp_space():
		logging.info('prepare_debian: not enough space in /tmp, using scratch space: %s %s %s', total_size, tmp_size(), tmp_space())
		tmp_parent = os.path.expanduser('~/tmp')
	elif dsc_name.startswith('libreoffice_') or dsc_name.startswith('iceweasel_'):
		logging.info('prepare: iceweasel/libreoffice, always using scratch space')
		tmp_parent = os.path.expanduser('~/tmp')
	else:
		logging.debug('prepare_debian: enough space in /tmp, not using scratch space: %s %s %s', total_size, tmp_size(), tmp_space())

	debian_tmp_dir = tempfile.mkdtemp(prefix='derivs-cmp-srcpkg-Debian-', dir=tmp_parent)

	# Setup the symlinks for dpkg-source/debdiff/etc
	for symlink_target, symlink_name in symlink_parts:
		symlink_path = os.path.join(debian_tmp_dir, symlink_name)
		os.symlink(symlink_target, symlink_path)

	return debian_tmp_dir

def get_changelog_entries(tmp_dir, dsc_name, dsc_sha1):
	logging.debug('getting changelog entries from %s', dsc_name)

	# Cache check
	changelog_path = hash_path(sha1_changelog_dir, dsc_sha1)
	if os.path.exists(changelog_path):
		logging.debug('changelog cache exists for %s %s', dsc_name, dsc_sha1)
		with open(changelog_path) as f:
			try: changelog_entries = json.load(f)
			except ValueError: pass
			else: return [tuple(entry) for entry in changelog_entries]

	# Preparation
	extract_path = os.path.join(tmp_dir,'extracted')

	# Unpack the source tree
	logging.debug('unpacking source package %s', dsc_name)
	cmdline = ['dpkg-source', '-x', dsc_name, 'extracted']
	process = subprocess.Popen(cmdline, cwd=tmp_dir, env=tmp_environ(tmp_dir), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, preexec_fn=subprocess_setup)
	output = process.communicate()[0]
	if process.returncode:
		logging.warning('dpkg-source reported failure to extract %s:', dsc_name)
		logging.warning(output)
		cmdline = ['ls', '-lR', '--time-style=+']
		process = subprocess.Popen(cmdline, cwd=tmp_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, preexec_fn=subprocess_setup)
		output = process.communicate()[0]
		logging.warning(output)
		rmtree(extract_path)
		return None

	# Sanitise the debian dir and changelog file in case it is a symlink to outside
	debian_dir = os.path.join(extract_path, 'debian')
	changelog_filename = os.path.join(debian_dir,'changelog')
	if os.path.islink(debian_dir) or os.path.islink(changelog_filename):
		logging.warning('debian dir or changelog is a symbolic link %s', dsc_name)
		rmtree(extract_path)
		return None

	# Check if the changelog exists
	if not os.path.exists(changelog_filename):
		logging.warning('could not find changelog in %s', dsc_name)
		rmtree(extract_path)
		return None

	# Find out which source package is the most likely derivative
	logging.debug('parsing changelog for %s', dsc_name)
	changelog_file = open(changelog_filename)
	try:
		changelog_obj = changelog.Changelog(changelog_file)
	except UnicodeDecodeError:
		changelog_file.seek(0)
		changelog_obj = changelog.Changelog(changelog_file, encoding='iso-8859-1')
	try:
		changelog_entries = [(entry.package, str(entry._raw_version)) for entry in changelog_obj]
	except:
		logging.warning('could not read changelog from %s', dsc_name)
		rmtree(extract_path)
		return None
	del changelog_obj
	changelog_file.close()

	# Clean up again
	rmtree(extract_path)

	# Write the cache
	makedirs(hash_path_parent(sha1_changelog_dir, dsc_sha1))
	remove(changelog_path)
	with open(changelog_path, 'w') as f:
		json.dump(changelog_entries, f)

	return changelog_entries

# Find the source package name and version this is probably derived from
def find_derived_from(tmp_dir, name, version, dsc_name, dsc_sha1, parts_unmodified):
	logging.debug('finding base source package of %s %s', name, version)

	# Get a list of changelog entries
	changelog_entries = get_changelog_entries(tmp_dir, dsc_name, dsc_sha1)
	if changelog_entries:
		logging.debug('changelog entries are: %s', ' '.join(['%s %s' % (entry_name, entry_version) for entry_name, entry_version in changelog_entries]))

	# Get a list of candidate versions from the database
	possibly_derived_from = []
	logging.debug('checking which parts were in Debian')
	for part_sha1, part_name in parts_unmodified:
		part_derived_from = sha1_to_srcpkgs(part_sha1)
		if part_derived_from:
			logging.debug('part %s %s available in %s', part_sha1, part_name, ' '.join(['%s %s' % (entry_name, entry_version) for entry_name, entry_version in part_derived_from]))
			possibly_derived_from.extend(part_derived_from)

	if not possibly_derived_from:
		logging.debug('no parts in common with Debian, obtaining old versions')
		old_packages = srcpkg_to_srcpkgs(name)
		if old_packages: possibly_derived_from = old_packages

	# Uniqify
	possibly_derived_from = list(set(possibly_derived_from))
	if possibly_derived_from:
		logging.debug('possibly derived from: %s', ' '.join(['%s %s' % (entry_name, entry_version) for entry_name, entry_version in possibly_derived_from]))
	else:
		logging.debug('nothing in possibly derived from list')

	# Match changelog versions against candidates
	if changelog_entries:
		logging.debug('matching changelog entries against versions possibly derived from')
		for entry in changelog_entries:
			entry_name, entry_version = entry
			if entry in possibly_derived_from:
				logging.debug('%s %s in possibly derived from', entry_name, entry_version)
				return entry
		logging.debug('checking if changelog entries were ever in Debian')
		for entry_name, entry_version in changelog_entries:
			if srcpkg_was_in_debian(entry_name, entry_version):
				logging.debug('%s %s was in Debian', entry_name, entry_version)
				return (entry_name, entry_version)
	if possibly_derived_from:
		logging.debug('finding closest entry in possibly derived from')
		possibly_derived_from.sort(cmp=lambda a,b: apt_version_cmp(b[1],a[1]))
		for entry_name, entry_version in possibly_derived_from:
			if name == entry_name and apt_version_cmp(version, entry_version) >= 0:
				logging.debug('%s %s is an equal or lower version', entry_name, entry_version)
				return (entry_name, entry_version)
		entry = possibly_derived_from[-1]
		entry_name, entry_version = entry
		logging.debug('no lower version numbers, returning next highest version %s %s', entry_name, entry_version)
		return entry
	logging.debug('finding closest version number in Debian')
	for entry in srcpkg_to_derived_from(name, version):
		entry_name, entry_version = entry
		logging.debug('closest package was %s %s', entry_name, entry_version)
		return entry
	logging.debug('could not find Debian package %s %s is derived from', name, version)
	return None

# Generate a patch file
def create_patch(tmp_dir, dsc_name, dsc_sha1, debian_tmp_dir, debian_dsc_name, debian_dsc_sha1):
	global repackaged_but_identical

	dsc_path = os.path.join(tmp_dir, dsc_name)
	debian_dsc_path = os.path.join(debian_tmp_dir, debian_dsc_name)
	path_everything = sha1_patch_path(debian_dsc_sha1, dsc_sha1)
	path_debian = sha1_patch_path(debian_dsc_sha1, dsc_sha1, 'debian')

	# Generate the main patch
	if not os.path.exists(path_everything) and os.path.exists(debian_dsc_path) and os.path.exists(dsc_path):
		makedirs(os.path.dirname(path_everything))
		cmdline = ['debdiff', '--quiet', '--diffstat', debian_dsc_path, dsc_path]
		stdout = open(path_everything, 'w')
		process = subprocess.Popen(cmdline, env=tmp_environ(tmp_dir), stdout=stdout, stderr=subprocess.PIPE, preexec_fn=subprocess_setup)
		output = process.communicate()[1]
		stdout.close()
		if process.returncode == 255:
			logging.warning('debdiff reported failure %s %s:', debian_dsc_name, dsc_name)
			logging.warning(output)
			cmdline = ['ls', '-lR', '--time-style=+']
			for name, dir in (derivative_short_name, tmp_dir), ('Debian', debian_tmp_dir):
				logging.warning('dir listing for %s:', name)
				process = subprocess.Popen(cmdline, cwd=dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, preexec_fn=subprocess_setup)
				output = process.communicate()[0]
				logging.warning(output)
			return False
		elif process.returncode == 0:
			logging.info('derivative repackaged in an identical way %s %s', debian_dsc_name, dsc_name)
			repackaged_but_identical += 1
			return False
		elif process.returncode != 1:
			logging.warning('debdiff reported unknown return code %s %s %s:', process.returncode, debian_dsc_name, dsc_name)
			logging.warning(output)
			cmdline = ['ls', '-lR', '--time-style=+']
			for name, dir in (derivative_short_name, tmp_dir), ('Debian', debian_tmp_dir):
				logging.warning('dir listing for %s:', name)
				process = subprocess.Popen(cmdline, cwd=dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, preexec_fn=subprocess_setup)
				output = process.communicate()[0]
				logging.warning(output)
			return False

	# Filter the main patch to include only the debian/ directory
	if os.path.exists(path_everything) and not os.path.exists(path_debian):
		makedirs(os.path.dirname(path_debian))
		cmdline = ['filterdiff', '--include=*/debian/*', path_everything]
		filterdiff = subprocess.Popen(cmdline, env=tmp_environ(tmp_dir), stdout=subprocess.PIPE, preexec_fn=subprocess_setup)
		filterdiff_output = filterdiff.communicate()[0]
		diffstat = subprocess.Popen('diffstat', stdin=subprocess.PIPE, stdout=subprocess.PIPE, preexec_fn=subprocess_setup)
		diffstat_output = diffstat.communicate(filterdiff_output)[0]
		with open(path_debian, 'w') as f:
			f.write('diffstat of debian/ for %s %s\n' % (os.path.splitext(debian_dsc_name)[0], os.path.splitext(dsc_name)[0]))
			f.write('\n')
			f.write(diffstat_output)
			f.write('\n')
			f.write(filterdiff_output)

	# Patches > 100MB are probably not that useful, replace them with a link
	for path in path_everything, path_debian:
		try:
			if os.path.getsize(path) > 104857600:
				logging.info('patch between %s and %s is larger than 100MB', dsc_name, debian_dsc_name)
				remove(path)
				symlink(os.path.relpath(patch_too_large, os.path.dirname(path)), path)
		except OSError:
			pass

	return True

def check_patch(debian_dsc_sha1, dsc_sha1):
	patch_path = sha1_patch_path(debian_dsc_sha1, dsc_sha1)
	lsdiff_path = sha1_lsdiff_path(debian_dsc_sha1, dsc_sha1)
	if os.path.exists(lsdiff_path):
		logging.debug('lsdiff cache exists for %s', patch_path)
		with open(lsdiff_path) as f:
			lsdiff = f.read()
	else:
		logging.debug('lsdiff cache does not exist for %s', patch_path)
		cmdline = ['lsdiff', patch_path]
		process = subprocess.Popen(cmdline, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, preexec_fn=subprocess_setup)
		lsdiff = process.communicate()[0]
		makedirs(os.path.dirname(lsdiff_path))
		with open(lsdiff_path,'w') as f:
			f.write(lsdiff)
	lsdiff = lsdiff.splitlines()
	for line in lsdiff:
		if line != 'debian/changelog' and not line.endswith('/debian/changelog'):
			logging.debug('patch changes files other than debian/changelog')
			return True
	if lsdiff:
		logging.debug('patch does not change files other than debian/changelog')
	else:
		logging.debug('patch does not change any files')
	return False

def present_patch(name, version, dsc_sha1, debian_name, debian_version,  debian_dsc_sha1):
	useful_patch = check_patch(debian_dsc_sha1, dsc_sha1)
	patches = []
	types = ('', 'debian')
	for type in types:
		ln_to = sha1_patch_path(debian_dsc_sha1, dsc_sha1, type)
		if not os.path.exists(ln_to):
			continue
		ln_from_deriv = deriv_patch_path(name, version, debian_name, debian_version, type)
		ln_from_global = global_patch_path(name, version, debian_name, debian_version, type)
		makedirs(os.path.dirname(ln_from_deriv))
		makedirs(os.path.dirname(ln_from_global))
		remove(ln_from_deriv)
		remove(ln_from_global)
		symlink(os.path.relpath(ln_to, os.path.dirname(ln_from_deriv)), ln_from_deriv)
		symlink(os.path.relpath(ln_to, os.path.dirname(ln_from_global)), ln_from_global)
		if useful_patch:
			patches.append(os.path.relpath(ln_from_global, os.path.abspath(global_patch_dir)))
	return tuple(patches)

# Functions that wrap other functions and decide what to do

def check_source_package(source_entry, srcpkg):
	global modifies_dsc_files

	try:
		name = None
		version = None
		dir = None
		name = srcpkg['Package']
		version = srcpkg['Version']
		dir = srcpkg['Directory']
		if '/' in name or name == '..':
			logging.warning('could not process source package %s %s: possibly malicious name', name, version)
			return None
		if '/' in version or version == '..':
			logging.warning('could not process source package %s %s: possibly malicious version', name, version)
			return None
		if '..' in dir.split('/'):
			logging.warning('could not process source package %s %s: possibly malicious dir: %s', name, version, dir)
			return None
	except KeyError:
		logging.warning('could not process source package %s %s', name, version)
		return None
	logging.debug('started processing source package %s %s', name, version)
	info = get_info(srcpkg)
	if not info:
		logging.warning('finished processing source package %s %s: could not get any info', name, version)
		return None
	dsc_hash_type, dsc_hash, dsc_name, parts = info
	if '/' in dsc_name or dsc_name == '..':
		logging.warning('could not process source package %s %s: possibly malicious dsc name %s', name, version, dsc_name)
		return None
	if not ishex(dsc_hash):
		logging.warning('could not process source package %s %s: possibly malicious dsc hash %s', name, version, dsc_hash)
		return None
	dsc_url = source_entry.archive_uri('%s/%s' % (dir, dsc_name))
	logging.debug('found dsc file: %s %s %s', dsc_hash_type, dsc_hash, dsc_url)
	dsc_status, dsc_sha1 = status(dsc_hash_type, dsc_hash, dsc_url)
	logging.debug('checked dsc status: %s %s %s', dsc_status, dsc_sha1, dsc_url)
	if dsc_status == 'unmodified':
		# Ignore the srcpkg since we know it is was in Debian
		# at one point and is hopefully therefore unmodified
		logging.debug('finished processing source package %s %s: dsc unmodified', name, version)
		return None
	else:
		files = [(dsc_sha1, dsc_hash_type, dsc_hash)]
		parts_unmodified = []
		parts_modified = []
		parts_unknown = []
		for part in parts:
			part_name = part['name']
			if '/' in part_name or part_name == '..':
				logging.warning('could not process source package %s %s: possibly malicious part name %s', name, version, part_name)
				return None
			part_url = source_entry.archive_uri('%s/%s' % (dir, part_name))
			part_hash_type, part_hash = [(k, v) for k, v in part.iteritems() if k not in ('name', 'size')][0]
			if not ishex(part_hash):
				logging.warning('could not process source package %s %s: possibly malicious part hash %s', name, version, part_hash)
				return None
			logging.debug('found part file: %s %s %s', part_hash_type, part_hash, part_url)
			part_status, part_sha1 = status(part_hash_type, part_hash, part_url)
			logging.debug('checked part status: %s %s %s', part_status, part_sha1, part_url)
			if 'sha1' not in part and part_sha1: part['sha1'] = part_sha1
			if part_status == 'unmodified': parts_unmodified.append((part_sha1, part_name))
			elif part_status == 'modified': parts_modified.append((part_sha1, part_name))
			else: parts_unknown.append((part_sha1, part_name))
			if part_status == 'modified': files.append((part_sha1, part_hash_type, part_hash))

		all_parts_unmodified = (len(parts_unmodified) == len(parts))
		parts_unmodified = list(set(parts_unmodified))
		logging.debug('source package status %s %s: dsc %s, %s parts unmodified, %s parts modified, %s parts unknown', name, version, dsc_status, len(parts_unmodified), len(parts_modified), len(parts_unknown))

		if all_parts_unmodified:
			# Ignore the srcpkg since we know all the parts were
			# in Debian at one point and ergo, it is unmodified
			logging.debug('finished processing source package %s %s: all non-dsc parts unmodified', name, version)
			if dsc_status == 'modified':
				logging.info('source package %s %s: unmodified, but dsc different', name, version)
				modifies_dsc_files += 1
			return (files, None, None, None)
		else:
			logging.debug('some parts modified, looking for derived version %s %s', name, version)
			if not dsc_sha1:
				logging.warning('finished processing source package %s %s: sha1 missing for dsc file', name, version)
				return (files, None, None, None)
			if parts_unknown:
				logging.warning('finished processing source package %s %s: sha1 missing for some parts', name, version)
				return (files, None, None, None)
			new = None
			link = None
			patch = None
			tmp_dir, size = prepare(dsc_name, dsc_sha1, parts)
			if not tmp_dir:
				logging.warning('source package %s %s: could not create temporary dir for deriv: %s', name, version, dsc_name)
				return (files, None, None, None)
			derived_from = find_derived_from(tmp_dir, name, version, dsc_name, dsc_sha1, parts_unmodified)
			if derived_from:
				debian_name, debian_version = derived_from
				link = (debian_name, debian_version, name, version, dsc_url)
				logging.debug('source package %s %s derived from %s %s', name, version, debian_name, debian_version)
				debian_files = srcpkg_to_files(debian_name, debian_version)
				if debian_files:
					debian_info = get_debian_info(debian_files)
					if debian_info:
						debian_dsc_sha1, debian_dsc_name, debian_parts = debian_info
						logging.debug('Debian source package %s %s dsc found %s %s', debian_name, debian_version, debian_dsc_name, debian_dsc_sha1)
						debian_tmp_dir = prepare_debian(debian_dsc_name, debian_dsc_sha1, debian_parts, size)
						if debian_tmp_dir:
							patch_created = create_patch(tmp_dir, dsc_name, dsc_sha1, debian_tmp_dir, debian_dsc_name, debian_dsc_sha1)
							if patch_created:
								patch_names = present_patch(name, version, dsc_sha1, debian_name, debian_version, debian_dsc_sha1)
								if patch_names:
									patch = (debian_name, debian_version, debian_dsc_sha1, name, version, dsc_sha1, [part[0] for part in parts_modified], patch_names)
								else:
									logging.debug('patch between %s %s and %s %s is probably not useful', debian_name, debian_version, name, version)
							rmtree(debian_tmp_dir)
						else:
							# This could be an issue with disk space, snapshots or a file that is not distributable
							logging.warning('source package %s %s: could not create temporary dir for Debian: %s %s', name, version, debian_name, debian_version)
					else:
						logging.warning('source package %s %s: could not get Debian info for %s %s: %s', name, version, debian_name, debian_version, debian_info)
				else:
					if srcpkg_was_in_debian(debian_name, debian_version):
						logging.warning('source package %s %s: snapshot database issue, no Debian files found', debian_name, debian_version)
					else:
						logging.warning('source package %s %s: derived from %s %s possibly bogus', name, version, debian_name, debian_version)
			else:
				new = (name, version, dsc_url)
			rmtree(tmp_dir)
			logging.debug('finished processing source package %s %s: all done', name, version)
			return (files, patch, link, new)

def process_sources(source_entries, lists_dir):
	files = []
	patches = []
	links = []
	new = []
	for source in source_entries:
		for source_entry in source:
			logging.debug('processing sources.list entry %s', source_entry.describe)
			fn = os.path.join(lists_dir, source_entry.describe.rstrip(')').rpartition('(')[2])
			try: f = file(fn)
			except IOError: continue
			for srcpkg in deb822.Sources.iter_paragraphs(f):
				actions = check_source_package(source_entry, srcpkg)
				if actions:
					action_files, action_patch, action_link, action_new = actions
					if action_files:
						files.append(action_files)
						logging.debug('action: return files %s', ' '.join([' '.join([str(item) for item in action]) for action in action_files]))
					if action_patch:
						patches.append(action_patch)
						logging.debug('action: return patches %s', ' '.join([' '.join(action) for action in action_patch]))
					if action_link:
						links.append(action_link)
						logging.debug('action: return links to modified source packages %s', ' '.join(action_link))
					if action_new:
						new.append(action_new)
						logging.debug('action: return links to new source packages %s', ' '.join(action_new))
				logging.debug('done')
				logging.debug('')
			f.close()
	return (files, patches, links, new)

logging.debug('processing distribution %s', derivative_short_name)

files, patches, links, new = process_sources(source_entries, lists_dir)

# Done with the database, close the connection
cur.close()
conn.close()

# Write out the results
filename = sys.argv[3]
data = files
if data:
	output_data = {}
	for package in data:
		for modified_file in package:
			sha1, hash_type, hash = modified_file
			if sha1 not in output_data:
				output_data[sha1] = {}
			if hash_type != 'sha1' and hash_type not in output_data[sha1]:
				output_data[sha1][hash_type] = hash
			elif hash_type != 'sha1' and hash != output_data[sha1][hash_type]:
				logging.warning('hashes mismatched: %s: %s %s != %s', sha1, hash_type, hash, output_data[sha1][hash_type])
	with open(os.path.abspath(filename), 'wb') as output:
		yaml.safe_dump(output_data, output)

filename = sys.argv[4]
data = patches
if data:
	if not os.path.exists(os.path.join(global_patch_dir,'HEADER.html')):
		symlink('../../doc/HEADER.patches.html',os.path.join(global_patch_dir,'HEADER.html'))
	if not os.path.exists(os.path.join(global_patch_dir,'.htaccess')):
		symlink('../../etc/htaccess.patches',os.path.join(global_patch_dir,'.htaccess'))
	if not os.path.exists(os.path.join(deriv_patch_dir,'HEADER.html')):
		symlink('../../../doc/HEADER.patches.html',os.path.join(deriv_patch_dir,'HEADER.html'))
	if not os.path.exists(os.path.join(global_patch_dir,'.htaccess')):
		symlink('../../../etc/htaccess.patches',os.path.join(deriv_patch_dir,'.htaccess'))
	output_data = []
	for item in data:
		debian_name, debian_version, debian_sha1, name, version, sha1, parts_sha1, patches = item
		item = {}
		item['debian_name'] = debian_name
		item['debian_version'] = debian_version
		item['debian_sha1'] = debian_sha1
		item['name'] = name
		item['version'] = version
		item['sha1'] = sha1
		item['patches'] = patches
		item['parts'] = parts_sha1
		output_data.append(item)
	with open (os.path.abspath(filename), 'wb') as output:
		yaml.safe_dump(output_data, output)
else:
	remove(filename)

filename = sys.argv[5]
data = links
if data:
	data = list(set(data))
	data.sort(cmp=lambda a,b: cmp(a[0],b[0]) or apt_version_cmp(a[1],b[1]) or cmp(a[2],b[2]) or apt_version_cmp(a[3],b[3]))
	output_data = {}
	for debian_name, debian_version, name, version, dsc_url in data:
		if debian_name not in output_data:
			output_data[debian_name] = {}
		if debian_version not in output_data[debian_name]:
			output_data[debian_name][debian_version] = []
		item = {}
		item['name'] = name
		item['version'] = version
		item['dsc'] = dsc_url
		output_data[debian_name][debian_version].append(item)
	with open (os.path.abspath(filename), 'wb') as output:
		yaml.safe_dump(output_data, output)
else:
	remove(filename)

filename = sys.argv[6]
data = new
if data:
	data = list(set(data))
	data.sort(cmp=lambda a,b: cmp(a[0],b[0]) or apt_version_cmp(a[1],b[1]) or cmp(a[2],b[2]))
	output_data = {}
	for name, version, dsc_url in data:
		if name not in output_data:
			output_data[name] = {}
		if version not in output_data[name]:
			output_data[name][version] = []
		output_data[name][version].append(str(dsc_url))
	with open(os.path.abspath(filename), 'wb') as output:
		yaml.safe_dump(output_data, output)
else:
	remove(filename)

logging.shutdown()
